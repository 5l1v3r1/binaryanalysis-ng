#!/usr/bin/python3

# Binary Analysis Next Generation (BANG!)
#
# This file is part of BANG.
#
# BANG is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License,
# version 3, as published by the Free Software Foundation.
#
# BANG is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public
# License, version 3, along with BANG.  If not, see
# <http://www.gnu.org/licenses/>
#
# Copyright 2018-2019 - Armijn Hemel
# Licensed under the terms of the GNU Affero General Public License
# version 3
# SPDX-License-Identifier: AGPL-3.0-only
#
# Gets a file and unpacks contents using standard functionality in
# Python 3 or some custom code and writes the contents to a temporary
# directory.

import sys
import os
import pathlib
import stat
import shutil
import datetime
import tempfile
import re
import hashlib
import string
import pickle
import json
import inspect
import collections
import functools
import mimetypes

# import modules needed for multiprocessing
import multiprocessing
import queue

# import some module for collecting statistics and information about
# the run time environment of the tool, plus of runs.
import logging
import platform
import getpass

# import module for database
import psycopg2

# import the local file with unpacking methods
import bangunpack
import bangandroid
import bangfilesystems
import bangmedia
import bangfilescans
import bangsignatures
from bangscanneroptions import BangScannerOptions

from reporter.picklereport import *
from reporter.jsonreport import *
from reporter.humanreadablereport import *


# For proximity matching use the TLSH module. This is not a standard
# module.
import tlsh

# store the maximum look ahead window. This is unlikely to matter, but
# just in case.
maxsignaturelength = max(map(lambda x: len(x), bangsignatures.signatures.values()))
maxsignaturesoffset = max(bangsignatures.signaturesoffset.values()) + maxsignaturelength

hash_algorithms = ['sha256', 'md5', 'sha1']

def compute_empty_hash_results():
    global hash_algorithms
    emptyhashes = {}
    results = {}

    # pre-store empty hashes
    for hashtocompute in hash_algorithms:
        emptyhashes[hashtocompute] = hashlib.new(hashtocompute)
        results[hashtocompute] = emptyhashes[hashtocompute].hexdigest()
    return results

emptyhashresults = compute_empty_hash_results()

class ScanJob:
    def __init__(self,filename, labels, parent, propagatedlabels, extradata):
        self.filename = filename
        self.labels = labels
        self.parent = parent
        self.propagatedlabels = propagatedlabels
        self.extradata = extradata
        self.type = None
        self._stat_file()
        self.filesize = self.stat.st_size
    def is_unpacking_root(self):
        # assumption: there is no situation where parent = None and 'root' not in labels?
        return self.parent is None
        # return 'root' in self.labels
    def get_type(self):
        return self.type
    def _stat_file(self):
        self.stat = os.stat(self.filename)
    def _is_symlink(self):
        r = self.filename.is_symlink()
        if r: self.type = 'symbolic link'
        return r
    def _is_socket(self):
        r = stat.S_ISSOCK(self.stat.st_mode)
        if r: self.type = 'socket'
        return r
    def _is_fifo(self):
        r = stat.S_ISFIFO(self.stat.st_mode)
        if r: self.type = 'fifo'
        return r
    def _is_block_device(self):
        r = stat.S_ISBLK(self.stat.st_mode)
        if r: self.type = 'block device'
        return r
    def _is_character_device(self):
        r = stat.S_ISCHR(self.stat.st_mode)
        if r: self.type = 'character device'
        return r
    def _is_directory(self):
        r = self.filename.is_dir()
        if r: self.type = 'directory'
        return r
    def _is_empty(self):
        r = self.stat.st_size == 0
        if r: self.type = 'empty'
        return r
    def not_scannable(self):
        return self._is_symlink() or \
                self._is_socket() or \
                self._is_fifo() or \
                self._is_block_device() or \
                self._is_character_device() or \
                self._is_directory() or \
                self._is_empty()

    def try_unpack_padding_file(self,unpacker,fileresult):
        # padding files don't need to be scanned
        if 'padding' in self.labels:
            unpacker.set_needs_unpacking(False)
            report = {
                'offset': 0,
                'size': self.filesize,
                'files': [],
            }
            fileresult.add_unpackedfile(report)
        else:
            unpacker.set_needs_unpacking(True)



class FileResult:
    def __init__(self,scanjob,lenunpackdirectory):
        self.fileresult = {
            'fullfilename': str(scanjob.filename)
        }
        if not scanjob.is_unpacking_root():
            self.fileresult['parent'] = str(scanjob.parent)[lenunpackdirectory:]
        relativefilename = str(scanjob.filename)[lenunpackdirectory:]
        self.fileresult['filename'] = relativefilename
    def set_labels(self,labels):
        """set file result labels to a list without duplicates."""
        self.fileresult['labels'] = list(set(labels))
    def set_filesize(self,size):
        self.fileresult['filesize'] = size
    def set_hashresult(self,hashtype,value):
        self.fileresult[hashtype] = value
    def init_unpacked_files(self):
        self.fileresult['unpackedfiles'] = []
    def add_unpackedfile(self,report):
        self.fileresult['unpackedfiles'].append(report)
    def set_mimetype(self,mimeres):
        if mimeres[0] is not None:
            self.fileresult['mimetype'] = mimeres[0]
            if mimeres[1] is not None:
                self.fileresult['mimetype encoding'] = mimeres[1]
    def get(self):
        return self.fileresult
    def get_filename(self):
        return self.fileresult['filename']
    def get_hash(self,algorithm='sha256'):
        return self.fileresult[algorithm]

class ScanEnvironment:
    tlshlabelsignore = set([
        'compressed', 'graphics', 'audio', 'archive',
        'filesystem', 'srec', 'ihex', 'padding',
        'database'])
    def __init__(self, maxbytes, readsize, createbytecounter,
            tlshmaximum, synthesizedminimum,logging, paddingname):
            self.maxbytes = maxbytes
            self.readsize = readsize
            self.createbytecounter = createbytecounter
            self.tlshmaximum = tlshmaximum
            self.synthesizedminimum = synthesizedminimum
            self.logging = logging
            self.paddingname = paddingname
    def get_readsize(self):
        return self.readsize
    def get_createbytecounter(self):
        return self.createbytecounter
    def get_tlsmaximum(self):
        return self.tlshmaximum
    def use_tlsh(self,filesize,labels):
        return ( 256 <= filesize <= self.tlshmaximum ) and self.tlshlabelsignore.isdisjoint(set(labels))
    def get_synthesizedminimum(self):
        return self.synthesizedminimum
    def get_paddingname(self):
        return self.paddingname
    def get_maxbytes(self):
        return self.maxbytes


class FileContentsComputer:
    def __init__(self,read_size):
        self.computers = []
        self.read_size = read_size
    def subscribe(self, input_computer):
        self.computers.append(input_computer)
    def read(self,filename):
        if all(c.supports_memoryview for c in self.computers):
            return self._read_with_memory_view(filename)
        else:
            return self._read_with_file_read(filename)
    def _read_with_file_read(self,filename):
        scanfile = open(filename, 'rb')
        scanfile.seek(0)
        for c in self.computers:
            c.initialize()
        data = scanfile.read(self.read_size)
        while data != b'':
            for c in self.computers:
                c.compute(data)
            data = scanfile.read(self.read_size)
        for c in self.computers:
            c.finalize()
        scanfile.close()
    def _read_with_memory_view(self,filename):
        scanfile = open(filename, 'rb')
        scanfile.seek(0)
        for c in self.computers:
            c.initialize()
        scanbytes = bytearray(self.read_size)
        bytes_read = scanfile.readinto(scanbytes)
        while bytes_read != 0:
            data = memoryview(scanbytes[:bytes_read])
            for c in self.computers:
                c.compute(data)
            bytes_read = scanfile.readinto(scanbytes)
        for c in self.computers:
            c.finalize()
        scanfile.close()

class IsTextComputer:
    supports_memoryview = True
    def __init__(self):
        self.is_text = True
    def initialize(self):
        pass
    def compute(self,data):
        self.is_text = self.is_text and \
            all(chr(x) in string.printable for x in data)
    def finalize(self):
        pass
    def get(self):
        return self.is_text

class TLSHComputer:
    supports_memoryview = False
    def __init__(self):
        pass
    def initialize(self):
        self.tlsh_hash = tlsh.Tlsh()
    def compute(self,data):
        self.tlsh_hash.update(data)
    def finalize(self):
        self.tlsh_hash.final()
    def get(self):
        return self.tlsh_hash.hexdigest()

class TLSHComputerMemoryView:
    supports_memoryview = True
    def __init__(self):
        pass
    def initialize(self):
        self.tlsh_hash = tlsh.Tlsh()
    def compute(self,data):
        self.tlsh_hash.update(data.tobytes())
    def finalize(self):
        self.tlsh_hash.final()
    def get(self):
        return self.tlsh_hash.hexdigest()

class ByteCounter:
    supports_memoryview = True
    def __init__(self):
        pass
    def initialize(self):
        self.bytecounter = collections.Counter(
            dict([(i,0) for i in range(0, 256)]))
    def compute(self,data):
        self.bytecounter.update(data)
    def finalize(self):
        pass
    def get(self):
        return self.bytecounter

class Hasher:
    supports_memoryview = True
    def __init__(self, hash_algorithms):
        self.hash_algorithms = hash_algorithms
    def initialize(self):
        self.hashes = dict( [ (a, hashlib.new(a))
            for a in self.hash_algorithms ] )
    def compute(self,data):
        for a in self.hashes:
            self.hashes[a].update(data)
    def finalize(self):
        self.hash_results = dict( [ (algorithm,computed_hash.hexdigest())
            for algorithm,computed_hash in self.hashes.items() ] )
    def get(self):
        return self.hash_results

class Unpacker:
    def __init__(self):
        # Invariant: lastunpackedoffset ==
        # last known position in file with successfully unpacked data
        # everything before this offset is unpacked and identified.
        self.lastunpackedoffset = -1
        self.unpackedrange = []
        self.needsunpacking = True
        # signature based unpacking?
        self.signaturesfound = []
        self.counterspersignature = {}
    def needs_unpacking(self):
        return self.needsunpacking
    def last_unpacked_offset(self):
        return self.lastunpackedoffset
    def unpacked_range(self):
        return self.unpackedrange
    def set_last_unpacked_offset(self,offset):
        self.lastunpackedoffset = offset
    def set_needs_unpacking(self,needsunpacking):
        self.needsunpacking = needsunpacking
    def append_unpacked_range(self,low,high):
        self.unpackedrange.append((low, high))
    def make_data_unpack_directory(self,filename,filetype,nr=1):
        while True:
            d = "%s-%s-%d" % (filename, filetype, nr)
            try:
                os.mkdir(d)
                self.dataunpackdirectory = d
                break
            # TODO: be more specific in exceptions to prevent infinite loops
            except:
                nr += 1
        return nr
    def remove_data_unpack_directory(self):
        os.rmdir(self.dataunpackdirectory)
    def remove_data_unpack_directory_tree(self):
        # Remove the unpacking directory, including any data that
        # might accidentily be there, so first change the
        # permissions of all the files so they can be safely.
        dirwalk = os.walk(self.dataunpackdirectory)
        for direntries in dirwalk:
            # make sure all subdirectories and files can
            # be accessed and then removed.
            for subdir in direntries[1]:
                subdirname = os.path.join(direntries[0], subdir)
                if not os.path.islink(subdirname):
                    os.chmod(subdirname,
                             stat.S_IRUSR | stat.S_IWUSR | stat.S_IXUSR)
            for filenameentry in direntries[2]:
                fullfilename = os.path.join(direntries[0], filenameentry)
                if not os.path.islink(fullfilename):
                    os.chmod(fullfilename,
                             stat.S_IRUSR | stat.S_IWUSR | stat.S_IXUSR)
        shutil.rmtree(self.dataunpackdirectory)

    def get_data_unpack_directory(self):
        return self.dataunpackdirectory
    def try_unpack_file_for_extension(self, filename, extension, temporarydirectory):
        try:
            self.make_data_unpack_directory(filename, bangsignatures.extensionprettyprint[extension])
            return bangunpack.unpack_file_with_extension(filename,extension, self.dataunpackdirectory, temporarydirectory)
        except AttributeError as ex:
            self.remove_data_unpack_directory()
            return None

    def open_scanfile(self,filename,maxbytes):
        self.scanfile = open(filename, 'rb')
        self.scanfile.seek(max(self.last_unpacked_offset(), 0))
        self.scanbytesarray = bytearray(maxbytes)
        self.scanbytes = memoryview(self.scanbytesarray)

    def get_current_offset_in_file(self):
        return self.scanfile.tell()

    def read_chunk_from_scanfile(self):
        self.offsetinfile = self.get_current_offset_in_file()
        self.bytesread = self.scanfile.readinto(self.scanbytesarray)

    def close_scanfile(self):
        self.scanfile.close()

    def seek_to_find_next_signature(self):
        if self.scanfile.tell() < self.lastunpackedoffset:
            # skip data that has already been unpacked
            self.scanfile.seek(self.lastunpackedoffset)
        else:
            # use an overlap, i.e. go back
            self.scanfile.seek(-maxsignaturesoffset, 1)


    def find_offsets_for_signature(self,s,filesize):
        offsets = set()
        # TODO: precompile regexp patterns in bangsignatures
        res = re.finditer(re.escape(bangsignatures.signatures[s]), self.scanbytes[:self.bytesread])
        if res is not None:
            for r in res:
                if s in bangsignatures.signaturesoffset:
                    # skip files that aren't big enough if the
                    # signature is not at the start of the data
                    # to be carved (example: ISO9660).
                    if r.start() + self.offsetinfile - bangsignatures.signaturesoffset[s] < 0:
                        continue

                offset = r.start()
                if not bangsignatures.prescan(s, self.scanbytes, self.bytesread, filesize, offset, self.offsetinfile):
                    continue

                # default: store a tuple (offset, signature name)
                if s in bangsignatures.signaturesoffset:
                    offsets.add((offset + self.offsetinfile - bangsignatures.signaturesoffset[s], s))
                else:
                    offsets.add((offset + self.offsetinfile, s))
        return offsets

    def offset_overlaps_with_unpacked_data(self,offset):
        return offset < self.lastunpackedoffset

    def try_unpack_file_for_signatures(self,filename):
        pass

    def file_unpacked(self, unpackresult,filesize):
        # store the location of where the successfully
        # unpacked file ends (the offset is always 0  here).
        self.lastunpackedoffset = unpackresult['length']

        # store the range of the unpacked data
        self.unpackedrange.append((0, unpackresult['length']))

        # if unpackedfilesandlabels is empty, then no files
        # were unpacked likely because the whole file was the
        # result and didn't contain any files (it was not a
        # container or compresed file)
        if unpackresult['filesandlabels'] == []:
            self.remove_data_unpack_directory()

        # whole file has already been unpacked, so no need for
        # further scanning.
        if unpackresult['length'] == filesize:
            self.needsunpacking = False


def log(level,message):
    global uselogging
    if uselogging:
        logging.log(level,message)

# Process a single file.
# This method has the following parameters:
#
# * scanfilequeue :: a queue where files to scan will be fetched from
# * resultqueue :: a queue where results will be written to
# * processlock :: a lock object that guards access to shared objects
# * checksumdict :: a shared dictionary to store hashes of files so
#   unnecessary scans of duplicate files can be prevented.
# * unpackdirectory :: the absolute path of the top level directory in
#   which files will be unpacked
# * resultsdirectory :: the absolute path of the directory where results
#   will be written to
# * temporary directory :: the absolute path of a directory in which
#   temporary files will be written
# * dbconn :: a PostgreSQL database connection
# * dbcursor :: a PostgreSQL database cursor
# * bangfilefunctions :: a list of functions for individual files
# * scanenvironment :: a dict that describes the environment in
#   which the scans run
#
# Each file will be in the scan queue and have the following data
# associated with it:
#
# * file name :: absolute path to the file to be scanned
# * set of labels :: either empty or containing hints from unpacking
# * parent :: name of parent file)
# * extradata :: empty, reserved for future use
#
# For every file a set of labels describing the file (such as 'binary' or
# 'graphics') will be stored. These labels can be used to feed extra
# information to the unpacking process, such as preventing scans from
# running.
def processfile(scanfilequeue, resultqueue, processlock, checksumdict,
                unpackdirectory, resultsdirectory, temporarydirectory,
                dbconn, dbcursor, bangfilefunctions, scanenvironment):
    global emptyhashresults
    global hash_algorithms

    lenunpackdirectory = len(str(unpackdirectory)) + 1

    createbytecounter = scanenvironment.get_createbytecounter()
    # uselogging = scanenvironment['logging']

    if dbconn is None:
        usedatabase = False
    else:
        usedatabase = True

    carveunpacked = True

    while True:
        # grab a new file from the scanning queue
        scanjob = scanfilequeue.get(timeout=86400)
        if not scanjob: continue
        # (filename, labels, parent, propagatedlabels, extradata) = scanfilequeue.get(timeout=86400)

        # store the results of the file
        # At minimum store:
        # * file name (relative to the top level unpack directory))
        # * labels
        fileresult = FileResult(scanjob,lenunpackdirectory)

        if scanjob.not_scannable():
            scanjob.labels.append(scanjob.get_type())
            fileresult.set_labels(scanjob.labels)
            if scanjob.get_type() == 'empty':
                fileresult.set_filesize(0)
                for hash_algorithm,hash_value in emptyhashresults.items():
                    fileresult.set_hashresult(hash_algorithm,hash_value)
            resultqueue.put(fileresult)
            scanfilequeue.task_done()
            continue

        # filesize = os.stat(scanjob.filename).st_size

        # create an empty list of files that were
        # unpacked/carved from this file
        fileresult.init_unpacked_files()

        unpacker = Unpacker()

        # remove any duplicate labels
        scanjob.labels = list(set(scanjob.labels))

        scanjob.try_unpack_padding_file(unpacker,fileresult)

        # check for a dummy value to see if the file has already been
        # unpacked and if so, simply report and skip the unpacking, and
        # move on to just running the per file scans.
        if 'unpacked' in scanjob.labels:
            scanjob.labels.remove('unpacked')
            unpacker.set_needs_unpacking(False)
            unpacker.set_last_unpacked_offset(scanjob.filesize)
            unpacker.append_unpacked_range(0, scanjob.filesize)

            # store lot of information about the unpacked files
            # TODO: add more information, such as signature
            report = {
                'offset': 0,
                'size': scanjob.filesize,
                'files': [],
            }
            fileresult.add_unpackedfile(report)

        # Search the extension of the file in a list of known extensions.
        # https://www.iana.org/assignments/media-types/media-types.xhtml
        mimeres = mimetypes.guess_type(scanjob.filename.name)
        fileresult.set_mimetype(mimeres)

        # Now try to unpack the file. There are a few categories:
        #
        # 1. a known extension, without a known magic header. This is
        #    for for example Android sparse data image formats, or
        #    several other Android or Google formats (Chrome PAK, etc.)
        #
        # 2. blobs, searching for known magic headers and carving them
        #    from blobs, or regular files. The majority of files will
        #    likely be processed here.
        #
        # 3. text only files, where it is not immediately clear what
        #    is inside and where the file possibly first has to be
        #    converted to a binary (examples: Intel Hex).

        # first process files with a known extension, but where
        # there is no clear magic header. A prime example is the
        # Android sparse data image format (system.new.dat and friends)

        # TODO: check if this previously missing check is ok here
        if unpacker.needs_unpacking():
            for extension in bangsignatures.extensiontofunction:
                if bangsignatures.matches_file_pattern(scanjob.filename,extension):
                    unpackresult = unpacker.try_unpack_file_for_extension(
                            scanjob.filename,extension,temporarydirectory)
                    if unpackresult is None:
                        continue
                    if not unpackresult['status']:
                        # No data could be unpacked for some reason
                        log(logging.DEBUG,"FAIL %s known extension %s: %s" %
                                (fileresult.get_filename(), extension,
                                unpackresult['error']['reason']))
                        # Fatal errors should lead to the program stopping
                        # execution. Ignored for now.
                        if unpackresult['error']['fatal']:
                            pass
                        unpacker.remove_data_unpack_directory_tree()
                        continue

                    # the file could be unpacked successfully,
                    # so log it as such.
                    log(logging.INFO,"SUCCESS %s %s at offset: 0, length: %d" %
                            (fileresult.get_filename(), extension,
                            unpackresult['length']))

                    unpacker.file_unpacked(unpackresult,scanjob.filesize)

                    # store any labels that were passed as a result and
                    # add them to the current list of labels
                    scanjob.labels += unpackresult['labels']
                    scanjob.labels = list(set(scanjob.labels))

                    # store lot of information about the unpacked files
                    report = {
                        'offset': 0,
                        'extension': extension,
                        'type': bangsignatures.extensionprettyprint[extension],
                        'size': unpackresult['length'],
                        'files': [],
                    }

                    for unpackedfile, unpackedlabel in unpackresult['filesandlabels']:
                        j = ScanJob(pathlib.Path(unpackedfile), unpackedlabel, scanjob.filename, [], {})
                        scanfilequeue.put(j)
                        report['files'].append(unpackedfile[len(unpacker.get_data_unpack_directory())+1:])
                    fileresult.add_unpackedfile(report)

            # end unpacking for extensions
            # -----------------------------------------------------------------

        # keep track of the signatures that were found, but
        # only for successful unpacking, or when no data could
        # be unpacked. Since the rest of the data could be unpacked
        # it is not interesting.
        signaturesfound = []

        # continue scanning if needed by searching for known signatures
        if unpacker.needs_unpacking():
            # keep a counter per signature for the unpacking
            # directory names
            counterspersignature = {}

            unpacker.open_scanfile(scanjob.filename, scanenvironment.get_maxbytes())
            unpacker.read_chunk_from_scanfile()

            # search the data for known signatures in the data that was read
            # TODO: check why this is a while true loop
            while True:
                candidateoffsetsfound = set()
                for s in bangsignatures.signatures:
                    offsets = unpacker.find_offsets_for_signature(s,scanjob.filesize)
                    candidateoffsetsfound = candidateoffsetsfound.union(offsets)

                # For each of the found candidates see if any
                # data can be unpacked. Process these in the order
                # in which the signatures were found in the file.
                for offset_with_signature in sorted(candidateoffsetsfound):
                    # skip offsets which are not useful to look at
                    # for example because the data has already been
                    # unpacked.
                    (offset, signature) = offset_with_signature
                    if unpacker.offset_overlaps_with_unpacked_data(offset):
                        continue

                    # first see if there actually is a method to unpack
                    # this type of file
                    if signature not in bangsignatures.signaturetofunction:
                        continue

                    signaturesfound.append(offset_with_signature)

                    # always change to the declared unpacking directory
                    os.chdir(unpackdirectory)
                    # then create an unpacking directory specifically
                    # for the signature including the signature name
                    # and a counter for the signature.
                    namecounter = counterspersignature.get(signature,0) + 1
                    namecounter = unpacker.make_data_unpack_directory(scanjob.filename,
                            bangsignatures.signatureprettyprint.get(signature, signature),
                            namecounter)

                    # run the scan for the offset that was found
                    # First log which identifier was found and
                    # at which offset for possible later analysis.
                    log(logging.DEBUG,"TRYING %s %s at offset: %d" %
                            (fileresult.get_filename(), signature, offset))

                    try:
                        unpackresult = bangsignatures.signaturetofunction[signature](scanjob.filename, offset, unpacker.get_data_unpack_directory(), temporarydirectory)
                    except AttributeError as e:
                        unpacker.remove_data_unpack_directory()
                        continue

                    if not unpackresult['status']:
                        # No data could be unpacked for some reason,
                        # so log the status and error message
                        log(logging.DEBUG,"FAIL %s %s at offset: %d: %s" %
                                (fileresult.get_filename(), signature, offset,
                                    unpackresult['error']['reason']))

                        # Fatal errors should lead to the program
                        # stopping execution. Ignored for now.
                        if unpackresult['error']['fatal']:
                            pass

                        unpacker.remove_data_unpack_directory_tree()
                        continue

                    # first rewrite the offset, if needed
                    offset = unpackresult.get('offset',offset)

                    # the file could be unpacked successfully,
                    # so log it as such.
                    log(logging.INFO,"SUCCESS %s %s at offset: %d, length: %d" %
                            (fileresult.get_filename(), signature, offset, unpackresult['length']))

                    # store the name counter
                    counterspersignature[signature] = namecounter

                    # store the labels for files that could be
                    # unpacked/verified completely.
                    if offset == 0 and unpackresult['length'] == scanjob.filesize:
                        scanjob.labels += unpackresult['labels']
                        scanjob.labels = list(set(scanjob.labels))
                        # if unpackedfilesandlabels is empty, then no
                        # files were unpacked, likely because the whole
                        # file was the result and didn't contain any
                        # files (i.e. it was not a container file or
                        # compressed file).
                        if unpackresult['filesandlabels'] == []:
                            unpacker.remove_data_unpack_directory()

                    # store the range of the unpacked data
                    unpacker.append_unpacked_range(offset, offset + unpackresult['length'])

                    # store lot of information about the unpacked files
                    report = {
                        'offset': offset,
                        'signature': signature,
                        'type': bangsignatures.signatureprettyprint.get(signature, signature),
                        'size': unpackresult['length'],
                        'files': [],
                    }

                    # set unpackdirectory, but only if needed: if the entire
                    # file is a file that was verified (example: GIF or PNG)
                    # then there will not be an unpacking directory.
                    if unpackresult['filesandlabels'] != []:
                        report['unpackdirectory'] = unpacker.get_data_unpack_directory()[lenunpackdirectory:]

                    for unpackedfile,unpackedlabel in unpackresult['filesandlabels']:
                        # TODO: make relative wrt unpackdir
                        report['files'].append(unpackedfile[len(unpacker.get_data_unpack_directory())+1:])
                        # add the data, plus possibly any label
                        j = ScanJob(pathlib.Path(unpackedfile), unpackedlabel, scanjob.filename, [], {})
                        scanfilequeue.put(j)

                    fileresult.add_unpackedfile(report)

                    # skip over all of the indexes that are now known
                    # to be false positives
                    unpacker.set_last_unpacked_offset(offset + unpackresult['length'])

                    # something was unpacked, so record it as such
                    unpacker.set_needs_unpacking(False)

                # check if the end of file has been reached, if so exit
                if unpacker.get_current_offset_in_file() == scanjob.filesize:
                    break

                unpacker.seek_to_find_next_signature()
                unpacker.read_chunk_from_scanfile()

            unpacker.close_scanfile()

        if carveunpacked:
            # Now carve any data that was not unpacked from the file and
            # put it back into the scanning queue to see if something
            # could be unpacked after all, or to more quickly recognize
            # padding data.
            #
            # This also makes it easier for doing a "post mortem".
            if unpacker.unpacked_range() != []:
                # first check if the first entry covers the entire file
                # because if so there is nothing to do
                if unpacker.unpacked_range()[0] != (0,scanjob.filesize):
                    synthesizedcounter = 1
                    startoffset = 0
                    scanfile = open(scanjob.filename, 'rb')
                    scanfile.seek(0)
                    # then try to see if the any useful data can be uncarved.
                    # Add an artifical entry for the end of the file
                    for u in unpacker.unpacked_range() + [(scanjob.filesize+1, scanjob.filesize+1)]:
                        if startoffset == scanjob.filesize:
                            break
                        if u[0] > startoffset:
                            #if u[0] - startoffset < scanenvironment.get_synthesizedminimum():
                            #        startoffset = u[1]
                            #        continue
                            dataunpackdirectory = "%s-%s-%d" % (scanjob.filename, "synthesized", synthesizedcounter)
                            try:
                                os.mkdir(dataunpackdirectory)
                            except:
                                # TODO: check this behaviour: file exist, break out of the loop??
                                break
                            synthesizedcounter += 1

                            outfilename = os.path.join(dataunpackdirectory, "unpacked-%s-%s" % (hex(startoffset), hex(u[0]-1)))
                            outfile = open(outfilename, 'wb')
                            os.sendfile(outfile.fileno(), scanfile.fileno(), startoffset, u[0] - startoffset)
                            outfile.close()

                            unpackedlabel = ['synthesized']

                            # try to see if the file contains NUL byte padding
                            # or 0xFF padding and if so tag it as such
                            ispadding = True
                            outfile = open(outfilename, 'rb')
                            checkbytes = outfile.read(1)
                            validpadding = [b'\x00', b'\xff']
                            if checkbytes in validpadding:
                                paddingchar = checkbytes
                                # now read more bytes
                                while True:
                                    scanbytes = outfile.read(scanenvironment.get_maxbytes())
                                    if scanbytes == b'':
                                        break
                                    if scanbytes != len(scanbytes) * paddingchar:
                                        ispadding = False
                                        break
                            else:
                                ispadding = False
                            outfile.close()
                            if ispadding:
                                unpackedlabel.append('padding')
                                if scanenvironment.get_paddingname() is not None:
                                    newoutfilename = os.path.join(dataunpackdirectory, "%s-%s-%s" % (scanenvironment.get_paddingname(), hex(startoffset), hex(u[0]-1)))
                                    shutil.move(outfilename, newoutfilename)
                                    outfilename = newoutfilename

                            startoffset = u[1]+1

                            # add the data, plus labels, to the queue
                            j = ScanJob(pathlib.Path(outfilename), unpackedlabel, scanjob.filename, [], {})
                            scanfilequeue.put(j)
                        startoffset = u[1]
                    scanfile.close()

        #---------------------------------------
        # compute various checksums of the file

        fc = FileContentsComputer(scanenvironment.get_readsize())
        hasher = Hasher(hash_algorithms)
        fc.subscribe(hasher)

        if createbytecounter and 'padding' not in scanjob.labels:
            byte_counter = ByteCounter()
            fc.subscribe(byte_counter)

        is_text = IsTextComputer()
        fc.subscribe(is_text)

        if scanenvironment.use_tlsh(scanjob.filesize,scanjob.labels):
            tlshc = TLSHComputerMemoryView()
            fc.subscribe(tlshc)

        fc.read(scanjob.filename)

        hashresults = dict(hasher.get())
        if scanenvironment.use_tlsh(scanjob.filesize,scanjob.labels):
            hashresults['tlsh'] = tlshc.get()
        for hash_algorithm,hash_value in hashresults.items():
            fileresult.set_hashresult(hash_algorithm, hash_value)

        # store if files are text or binary
        if is_text.get():
            scanjob.labels.append('text')
        else:
            scanjob.labels.append('binary')

        # no files were unpacked, so try to scan the entire file.
        # This happens for example with text only files like base64
        # or Intel hex (all ASCII only).
        if unpacker.needs_unpacking():
            if 'text' in scanjob.labels and unpacker.unpacked_range() == []:
                # finally try all the "text only" functions
                for f in bangsignatures.textonlyfunctions:
                    namecounter = 1
                    dataunpackdirectory = "%s-%s-%d" % (scanjob.filename, f, namecounter)
                    while True:
                        try:
                            os.mkdir(dataunpackdirectory)
                            break
                        except:
                            namecounter += 1

                    log(logging.DEBUG,"TRYING %s %s at offset: 0" % (fileresult.get_filename(), f))
                    try:
                        unpackresult = bangsignatures.textonlyfunctions[f](scanjob.filename, 0, dataunpackdirectory, temporarydirectory)
                    except Exception as e:
                        os.rmdir(dataunpackdirectory)
                        continue

                    if not unpackresult['status']:
                        # No data could be unpacked for some reason,
                        # so check the status first
                        log(logging.DEBUG,"FAIL %s %s at offset: %d: %s" %
                                (fileresult.get_filename(), f, 0, unpackresult['error']['reason']))
                        #print(s[1], unpackresult['error'])
                        #sys.stdout.flush()
                        # unpackerror contains:
                        # * offset in the file where the error occured
                        #   (integer)
                        # * reason of the error (human readable)
                        # * flag to indicate if it is a fatal error
                        #   (boolean)
                        #
                        # Fatal errors should stop execution of the
                        # program and remove the unpacking directory,
                        # so first change the permissions of
                        # all the files so they can be safely removed.
                        if unpackresult['error']['fatal']:
                            pass
                        # clean up any data that might have been left behind
                        dirwalk = os.walk(dataunpackdirectory)

                        for direntries in dirwalk:
                            # make sure all subdirectories and files
                            # can be accessed
                            for subdir in direntries[1]:
                                subdirname = os.path.join(direntries[0], subdir)
                                if not os.path.islink(subdirname):
                                    os.chmod(subdirname, stat.S_IRUSR | stat.S_IWUSR | stat.S_IXUSR)
                            for filenameentry in direntries[2]:
                                fullfilename = os.path.join(direntries[0], filenameentry)
                                if not os.path.islink(fullfilename):
                                    os.chmod(fullfilename, stat.S_IRUSR | stat.S_IWUSR | stat.S_IXUSR)
                        shutil.rmtree(dataunpackdirectory)
                        continue

                    log(logging.INFO,"SUCCESS %s %s at offset: %d, length: %d" %
                            (fileresult.get_filename(), f, 0, unpackresult['length']))

                    # store the labels for files that could be
                    # unpacked/verified completely.
                    if unpackresult['length'] == scanjob.filesize:
                        scanjob.labels += unpackresult['labels']
                        scanjob.labels = list(set(scanjob.labels))
                        # if unpackedfilesandlabels is empty, then no
                        # files were unpacked, likely because the whole
                        # file was the result and didn't contain any
                        # files (i.e. it was not a container file or
                        # compresed file).
                        #if len(unpackresult['filesandlabels']) == 0:
                        if unpackresult['filesandlabels'] == []:
                            os.rmdir(dataunpackdirectory)

                    # store lot of information about the unpacked files
                    report = {
                        'offset': 0,
                        'signature': f,
                        'type': f,
                        'size': unpackresult['length'],
                        'files': [],
                    }

                    unpacker.set_last_unpacked_offset(unpackresult['length'])
                    unpacker.append_unpacked_range(0, unpackresult['length'])

                    for un in unpackresult['filesandlabels']:
                        (unpackedfile, unpackedlabel) = un

                        # TODO: make relative wrt unpackdir
                        report['files'].append(unpackedfile[len(dataunpackdirectory)+1:])

                        # add the data, plus possibly any label
                        j = ScanJob(pathlib.Path(unpackedfile), unpackedlabel, scanjob.filename, [], {})
                        scanfilequeue.put(j)

                    fileresult.add_unpackedfile(report)
                    break

        # run individual scans for files and store them in a separate
        # result file, but only do this once for every file, so check
        # the shared dictionary with checksum results first.
        duplicate = False

        processlock.acquire()
        # TODO: make checksumdict an object
        # if hashresults['sha256'] in checksumdict:
        if fileresult.get_hash() in checksumdict:
            duplicate = True
        else:
            checksumdict[fileresult.get_hash()] = scanjob.filename
        processlock.release()

        if not duplicate:
            # Run scans for each individual file. Parameters include:
            # * filename (pathlib.Path object)
            # * hashes (dict 'hashresults')

            for b in bangfilefunctions:
                runscan = True
                ignorelist = set(b[1])
                filefunc = b[0]
                for label in scanjob.labels:
                    if label in ignorelist:
                        runscan = False
                        break
                if runscan:
                    res = filefunc[1](scanjob.filename, hashresults, dbconn, dbcursor, scanenvironment)

            # write a pickle with output data
            # The pickle contains:
            # * all available hashes
            # * labels
            # * byte count
            # * any extra data that might have been passed around
            resultout = {}
            if createbytecounter and 'padding' not in scanjob.labels:
                resultout['bytecount'] = sorted(byte_counter.get().items())
                # write a file with the distribution of bytes in the scanned file
                bytescountfilename = resultsdirectory / ("%s.bytes" % fileresult.get_hash())
                if not bytescountfilename.exists():
                    bytesout = bytescountfilename.open('w')
                    for by in resultout['bytecount']:
                        bytesout.write("%d\t%d\n" % by)
                    bytesout.close()

            for i in hashresults:
                resultout[i] = hashresults[i]

            resultout['labels'] = scanjob.labels
            if scanjob.extradata != {}:
                resultout['extra'] = scanjob.extradata
            picklefilename = resultsdirectory / ("%s.pickle" % fileresult.get_hash('sha256'))
            if not picklefilename.exists():
                pickleout = picklefilename.open('wb')
                pickle.dump(resultout, pickleout)
                pickleout.close()

        else:
            scanjob.labels.append('duplicate')

        fileresult.set_labels(scanjob.labels)
        fileresult.set_filesize(scanjob.filesize)
        log(logging.INFO,json.dumps(fileresult.get()))
        sys.stdout.flush()

        resultqueue.put(fileresult)
        scanfilequeue.task_done()

def connect_to_bang_database(options):
    return psycopg2.connect(database=options.postgresql_db, user=options.postgresql_user,
        password=options.postgresql_password,
        port=options.postgresql_port, host=options.postgresql_host)

def main(argv):
    global uselogging
    options = BangScannerOptions().get()

    # test the database. If the connection fails it depends on whether
    # or not it should be treated as a fatal error. If not, then
    # continue without using any of the database functionality.
    if options.usedatabase:
        try:
            c = connect_to_bang_database(options)
            c.close()
        except Exception as e:
            if options.dbconnectionerrorfatal:
                print("Configuration file malformed: missing or wrong database information", file=sys.stderr)
                sys.exit(1)
            options.usedatabase = False

    # tuple of database connection/database cursor
    bangdbconns = []

    # create database connections if any database is used
    if options.usedatabase:
        for i in range(0, options.bangthreads):
            bangconn = connect_to_bang_database(options)
            bangcursor = bangconn.cursor()
            bangdbconns.append((bangconn, bangcursor))

    # first determine how many bytes should be scanned for known
    # signatures using a sliding window. This should not be set too
    # large for performance reasons and not too low (to avoid a
    # silly window). Ideally this is a few times the value of
    # 'maxsignaturesoffset'
    maxbytes = max(200000, maxsignaturesoffset+1)

    # grab all the functions from the "bangfilescans" file
    bangfunctions = inspect.getmembers(bangfilescans, inspect.isfunction)

    # split them functions in file functions and others
    bangfilefunctions = []
    bangwholecontextfunctions = []

    for f in bangfunctions:
        context = None
        ignorelist = []
        if f[1].__doc__ is None:
            bangfilefunctions.append((f, []))
        else:
            for l in f[1].__doc__.split('\n'):
                if l.strip().startswith('Context:'):
                    context = l.split(':', 1)[1].strip()
                elif l.strip().startswith('Ignore:'):
                    ignoretypes = l.strip().split(',')
                    for i in ignoretypes:
                        ignorelist.append(i.strip())
            if context == 'file':
                bangfilefunctions.append((f, ignorelist))
            elif context == 'whole':
                bangwholecontextfunctions.append((f, ignorelist))

    checkfiles = []
    if os.path.isdir(options.checkpath):
        dirwalk = os.walk(options.checkpath)
        for i in dirwalk:
            for j in i[2]:
                scanfilename = os.path.join(i[0], j)
                if not os.path.exists(scanfilename):
                    continue

                # ... and should be a real file
                if not stat.S_ISREG(os.stat(scanfilename).st_mode):
                    continue

                filesize = os.stat(scanfilename).st_size
                # Don't scan an empty file
                if filesize == 0:
                    continue
                checkfiles.append(scanfilename)
    else:
        checkfiles.append(options.checkpath)

    if not checkfiles:
        print("No files to scan found, exiting", file=sys.stderr)
        sys.exit(1)

    if options.uselogging:
        uselogging = True
        # set up logging
        logging.basicConfig(level=logging.DEBUG, format='%(asctime)s %(message)s')
        banglogger = logging.getLogger()

        # remove the standard log handler, as it will be different
        # per scan directory
        for i in banglogger.handlers:
            banglogger.removeHandler(i)

    for checkfile in sorted(checkfiles):
        # Now the real scanning starts.
        scandate = datetime.datetime.utcnow()

        # create a directory for the scan
        scandirectory = pathlib.Path(tempfile.mkdtemp(prefix='bang-scan-',
                                                      dir=options.baseunpackdirectory))

        # create an empty file "STARTED" to easily identify
        # active (or crashed) scans.
        startedfile = open(scandirectory / "STARTED", 'wb')
        startedfile.close()

        # now create a directory structure inside the scandirectory:
        # unpack/ -- this is where all the unpacked data will be stored
        # results/ -- this is where files describing the unpacked data
        #             will be stored
        # logs/ -- this is where logs from the scan will be stored
        unpackdirectory = scandirectory / "unpack"
        unpackdirectory.mkdir()

        resultsdirectory = scandirectory / "results"
        resultsdirectory.mkdir()

        if uselogging:
            logdirectory = scandirectory / "logs"
            logdirectory.mkdir()

            # create a log file inside the log directory and
            # add it to the BANG logger
            bangloghandler = logging.FileHandler(filename=logdirectory / 'unpack.log')
            banglogger.addHandler(bangloghandler)
        log(logging.INFO,"Started scanning %s" % checkfile)

        processmanager = multiprocessing.Manager()

        # first create two queues: one for scanning files, the other one
        # for reporting results.
        scanfilequeue = processmanager.JoinableQueue(maxsize=0)
        resultqueue = processmanager.JoinableQueue(maxsize=0)
        processes = []

        # copy the file that needs to be scanned to the temporary
        # directory.
        try:
            shutil.copy(checkfile, unpackdirectory)
        except:
            print("Could not copy %s to scanning directory %s" % (checkfile, unpackdirectory), file=sys.stderr)
            log(logging.WARNING, "Could not copy %s to scanning directory" % checkfile)
            log(logging.INFO, "Finished scanning %s" % checkfile)
            # move the file "STARTED" to "FINISHED" to easily identify
            # active (or crashed) scans
            shutil.move(scandirectory / "STARTED",
                        scandirectory / "FINISHED")
            os.utime(scandirectory / "FINISHED")

            if options.removescandirectory:
                shutil.rmtree(scandirectory)
            continue

        # The scan queue will be used to put files into that need to be
        # scanned and processes. New files wil keep being added to it
        # while results are being unpacked recursively.
        # Initially one file will be in this queue, namely the first file.
        # After files are unpacked they will be added to the queue, as they
        # can be scanned in a trivially parallel way.

        # Create a list of labels to pass around. The first element is
        # tagged as 'root', as it is the root of the unpacking tree.
        labels = ['root']
        j = ScanJob(unpackdirectory / os.path.basename(checkfile), labels, None, [], {})
        scanfilequeue.put(j)

        # create a lock to control access to any shared data structures
        processlock = multiprocessing.Lock()

        # create a shared dictionary
        checksumdict = processmanager.dict()

        # create a scan environment for the new scan
        scanenvironment = ScanEnvironment(
            # set the maximum size for the amount of bytes to be read
            maxbytes = maxbytes,
            # set the size of bytes to be read during scanning hashes
            readsize = 10240,
            createbytecounter = options.createbytecounter,
            tlshmaximum = options.tlshmaximum,
            synthesizedminimum = 10,
            logging = uselogging,
            paddingname = 'PADDING')

        # create processes for unpacking archives
        for i in range(0, options.bangthreads):
            if not options.usedatabase:
                dbconn0 = None
                dbconn1 = None
            else:
                dbconn0 = bangdbconns[i][0]
                dbconn1 = bangdbconns[i][1]
            p = multiprocessing.Process(target=processfile,
                                        args=(scanfilequeue,
                                              resultqueue,
                                              processlock,
                                              checksumdict,
                                              unpackdirectory,
                                              resultsdirectory,
                                              options.temporarydirectory,
                                              dbconn0,
                                              dbconn1,
                                              bangfilefunctions,
                                              scanenvironment))
            processes.append(p)

        # then start all the processes
        for p in processes:
            p.start()

        # wait for the queues to be empty.
        scanfilequeue.join()

        # There is one result for each file in the result
        # queue, which need to be merged into a structure
        # matching the directory tree that was unpacked. The name
        # of each file that is unpacked serves as key into
        # the structure.
        scantree = {}

        while True:
            try:
                fileresult = resultqueue.get_nowait()
                try:
                    scantree[fileresult.get_filename()] = fileresult.get()
                    resultqueue.task_done()
                except KeyError:
                    # TODO: this is never the case, is it?
                    pass

            except queue.Empty as e:
                # Queue is empty
                break

        resultqueue.join()

        # Done processing, terminate processes that were created
        for p in processes:
            p.terminate()

        scandatefinished = datetime.datetime.utcnow()

        # move the file "STARTED" to "FINISHED" to easily identify
        # active (or crashed) scans
        shutil.move(scandirectory / "STARTED",
                    scandirectory / "FINISHED")
        os.utime(scandirectory / "FINISHED")

        # now store the scan tree results with other data
        scanresult = {
            'scantree': scantree,
            # statistics about this particular session
            'session': {
                    'start': scandate,
                    'stop': scandatefinished,
                    'duration': (scandatefinished - scandate).total_seconds(),
                    'user': getpass.getuser(),
                    'checkfile': checkfile,
            },
            'platform' : {
                    'machine': platform.machine(),
                    'architecture': platform.architecture()[0],
                    'processor': platform.processor(),
                    'node': platform.node(),
                    'system': platform.system(),
                    'release': platform.release(),
                    'libc': platform.libc_ver()[0],
                    'libcversion': platform.libc_ver()[1],
            },
            # some information about the used Python version
            'python': {
                    'version': platform.python_version(),
                    'implementation': platform.python_implementation(),
            },
        }


        # write all results to a Python pickle
        picklefile = open(scandirectory / 'bang.pickle', 'wb')
        PickleReporter(picklefile).report(scanresult)
        picklefile.close()

        jsonfile = open(scandirectory / 'bang.json','w')
        JsonReporter(jsonfile).report(scanresult)
        jsonfile.close()

        if options.writereport:
            reportfile = open(scandirectory / 'report.txt', 'w')
            HumanReadableReporter(reportfile).report(scanresult)
            reportfile.close()

        # The end.
        log(logging.INFO, "Finished scanning %s" % checkfile)
        if options.uselogging:
            # flush any remaining data
            bangloghandler.flush()

            # remove the log file from the system logger
            banglogger.removeHandler(bangloghandler)
            bangloghandler.close()

        if options.removescandirectory:
            shutil.rmtree(scandirectory)

    # clean up the database cursors and
    # close all connections to the database
    for c in bangdbconns:
        # first close the cursor
        c[1].close()
        # then close the database connection
        c[0].close()

    # finally shut down logging
    logging.shutdown()


if __name__ == "__main__":
    main(sys.argv)
